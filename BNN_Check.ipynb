{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Code to create Bayesian Neural Network (BNN) on example dataset\n",
        "# Ashok K Sharma"
      ],
      "metadata": {
        "id": "QZZJbg9AxJPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYu94AFDdF0X"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import csv as csv\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_map(input_data):\n",
        "    # Get the unique elements and their counts\n",
        "    unique, counts = np.unique(input_data, return_counts=True)\n",
        "    # Create a dictionary mapping each unique element to its count\n",
        "    count_dict = dict(zip(unique, counts))\n",
        "    # Sort the dictionary and return it\n",
        "    return sorted(count_dict.items())\n",
        "\n",
        "# Read DataSET Again becuase for NN and BNN data was converted in to Tensor Objects\n",
        "# Read the train and test datasets\n",
        "train_data = pd.read_csv(\"Example_data/train.csv\")\n",
        "test_data = pd.read_csv(\"Example_data/test.csv\")\n",
        "\n",
        "# Select the desired features from the datasets\n",
        "features = ['ClogP', 'BSEP', 'Glu', 'Glu_Gal', 'THLE', 'HepG2', 'Fsp3', 'log10cmax']\n",
        "X_train_df = train_data[features]\n",
        "X_test_df = test_data[features]\n",
        "\n",
        "# Initialize a StandardScaler to standardize the features to have mean=0 and variance=1\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler on the training data and transform both training and test data\n",
        "scaler.fit(X_train_df)\n",
        "X_train = pd.DataFrame(scaler.transform(X_train_df), columns=X_train_df.columns)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test_df), columns=X_test_df.columns)\n",
        "\n",
        "# Select the target variable\n",
        "target = ['dili_sev']\n",
        "Y_train = (train_data[target].values.astype(int) - 1)\n",
        "Y_test = (test_data[target].values.astype(int) - 1)\n",
        "\n",
        "# Apply the sort_map function on the target variable\n",
        "sorted_train_output = sort_map(Y_train)\n",
        "sorted_test_output = sort_map(Y_test)\n",
        "\n",
        "# Print the sorted output\n",
        "print(sorted_train_output)\n",
        "print(sorted_test_output)"
      ],
      "metadata": {
        "id": "z__hsz1oednU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance Scores\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Calculate mutual information\n",
        "mi_scores = mutual_info_classif(X_train, Y_train.ravel())\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "mi_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Mutual Information': mi_scores})\n",
        "mi_scores_df = mi_scores_df.sort_values(by='Mutual Information', ascending=False)\n",
        "\n",
        "# Print the most informative features\n",
        "print(mi_scores_df)"
      ],
      "metadata": {
        "id": "tlc3CroeANlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Neural Network"
      ],
      "metadata": {
        "id": "DBppnnYmAlny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--  Bayesian Neural Network with Defined Number of features, Neurons and Number of Classes\n",
        "#---- Define Following things\n",
        "n0 = 8 # Number of Features\n",
        "n1 = 15 # Number of Neurons in the Hidden Layer\n",
        "K = 3 # Number of Classes\n",
        "\n",
        "import pymc3 as pm\n",
        "import numpy as np\n",
        "import theano.tensor as tt\n",
        "\n",
        "# Define the BNN model using a feed-forward function\n",
        "def feedforward(inp, W0, b0, W1, b1):\n",
        "    layer1 = tt.nnet.relu(tt.dot(inp, W0) + b0)\n",
        "    output = tt.dot(layer1, W1) + b1\n",
        "    return output\n",
        "\n",
        "# Build the PyMC3 model\n",
        "with pm.Model() as model:\n",
        "    sig_w = pm.TruncatedNormal(\"sig_w\", mu=0, sigma=1, lower=0)\n",
        "    sig_b = pm.TruncatedNormal(\"sig_b\", mu=0, sigma=1, lower=0)\n",
        "\n",
        "    W0_flat = pm.Normal(\"W0_flat\", mu=0, sigma=sig_w, shape=(n0 * n1))\n",
        "    W0 = pm.Deterministic(\"W0\", W0_flat.reshape((n0, n1)))\n",
        "\n",
        "    b0 = pm.Normal(\"b0\", mu=0, sigma=sig_b, shape=n1)\n",
        "\n",
        "    W1_flat = pm.Normal(\"W1_flat\", mu=0, sigma=sig_w, shape=(n1 * K))\n",
        "    W1 = pm.Deterministic(\"W1\", W1_flat.reshape((n1, K)))\n",
        "\n",
        "    b1 = pm.Normal(\"b1\", mu=0, sigma=sig_b, shape=K)\n",
        "\n",
        "    # Define the likelihood using the feed-forward function\n",
        "    preds = feedforward(X_train, W0, b0, W1, b1)\n",
        "    # Define the likelihood\n",
        "    for i in range(len(Y_train)):\n",
        "        pm.Categorical(\"Y_train_{}\".format(i), p=softmax(preds[i]), observed=Y_train[i])\n",
        "\n",
        "    # Generate predictions for the test dataset\n",
        "    preds_test = feedforward(X_test, W0, b0, W1, b1)\n",
        "    # Define the likelihood\n",
        "    for i in range(len(Y_test)):\n",
        "        pm.Categorical(\"Y_test_{}\".format(i), p=softmax(preds_test[i]), observed=Y_test[i])\n",
        "\n",
        "    #output_train = feedforward(X_train, W0, b0, W1, b1)\n",
        "    #likelihood_train = pm.Normal(\"Y_train\", mu=output_train, sigma=1, observed=Y_train)\n",
        "\n",
        "    # Generate predictions for the test dataset\n",
        "    #output_test = feedforward(X_test, W0, b0, W1, b1)\n",
        "    #likelihood_test = pm.Normal(\"Y_test\", mu=output_test, sigma=1, observed=Y_test)\n",
        "\n",
        "    # Perform sampling\n",
        "    n_steps = 20_000\n",
        "    #n_tune = 1_000 # This was added by me\n",
        "    target_accept_rate = 0.65\n",
        "    #target_accept_rate = 0.9 #(Try if Required - Higher Target Acceptence Rates)\n",
        "    #trace = pm.sample(draws=n_steps, tune=n_tune, target_accept=target_accept_rate)\n",
        "    trace = pm.sample(draws=n_steps, target_accept=target_accept_rate)"
      ],
      "metadata": {
        "id": "_N998yktAfEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Posterior Predictions\n",
        "posterior_pred = pm.sample_posterior_predictive(trace, samples = 1000, model=model)"
      ],
      "metadata": {
        "id": "WTQ_2MR8AvoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model and Predictions - On Time Task\n",
        "import pickle\n",
        "\n",
        "#***************** Save Trace and Model\n",
        "# Save the trace\n",
        "#with open(\"Final_models/SemenovaData_trace_BNN_updated.pkl\", \"wb\") as file:\n",
        "#    pickle.dump(trace, file)\n",
        "# Save the model\n",
        "#with open(\"Final_models/SemenovaData_model_BNN_updated.pkl\", \"wb\") as file:\n",
        "#    pickle.dump(model, file)\n",
        "\n",
        "#import numpy as np\n",
        "#np.save('Final_models/posterior_prediction_BNN_updated.npy', posterior_pred)"
      ],
      "metadata": {
        "id": "19jHYHpcAz1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Predictions - On Time Task\n",
        "\n",
        "# Load the trace\n",
        "#with open(\"Final_models/SemenovaData_trace_BNN_updated.pkl\", \"rb\") as file:\n",
        "#    trace = pickle.load(file)\n",
        "\n",
        "# Load the model\n",
        "#with open(\"Final_models/SemenovaData_model_BNN_updated.pkl\", \"rb\") as file:\n",
        "#    model = pickle.load(file)\n",
        "\n",
        "# Load the Posterior Predicitons\n",
        "#posterior_pred = np.load('Final_models/posterior_prediction_BNN_updated.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "BhgQKYZOA46C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Performance on the Training SET"
      ],
      "metadata": {
        "id": "Gl01xy0wBd8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create an empty list to store the class label arrays\n",
        "class_labels = []\n",
        "\n",
        "# Iterate over the class labels\n",
        "for i in range(147):\n",
        "    class_label = posterior_pred[f'Y_train_{i}']\n",
        "    class_labels.append(class_label)\n",
        "\n",
        "# Stack the class label arrays horizontally\n",
        "train_predictions_array = np.hstack(class_labels)\n",
        "\n",
        "# Print the shape of the predictions array - on Train Dataset\n",
        "print(train_predictions_array.shape)\n",
        "\n",
        "train_predictions_array"
      ],
      "metadata": {
        "id": "0PObALcGA8oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# posterior prediction for each category\n",
        "def probs(y_pred_df, ind):\n",
        "    y_pred_ind = y_pred_df[:,ind]\n",
        "    p = [sum(y_pred_ind == 0)/len(y_pred_ind), sum(y_pred_ind == 1)/len(y_pred_ind), sum(y_pred_ind == 2)/len(y_pred_ind)]\n",
        "    return p\n",
        "\n",
        "# compute posterior probabilities for a selected drug\n",
        "print(probs(train_predictions_array, 0))\n",
        "print(probs(train_predictions_array, 1))\n",
        "print(probs(train_predictions_array, 2))"
      ],
      "metadata": {
        "id": "s9pMtgYgBZcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize predictions On Training Data Set\n",
        "train_predictions_array\n",
        "train_predictions_array.shape # 1000 times 147 Columns (Data) - For Each compound get which class got majority votes\n",
        "train_predictions_array_df = pd.DataFrame(train_predictions_array)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Convert the NumPy array to a DataFrame\n",
        "train_pred_df = pd.DataFrame(train_predictions_array_df)\n",
        "\n",
        "# Get the label with the majority of votes for each column\n",
        "train_majority_labels = np.asarray(mode(train_pred_df, axis=0)[0])[0]\n",
        "\n",
        "# Print the majority labels\n",
        "print(train_majority_labels)\n",
        "\n",
        "# Just check\n",
        "train_predictions_array_df[0].value_counts()"
      ],
      "metadata": {
        "id": "f1Z3el1PBomI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_actual_pred = np.column_stack((Y_train, train_majority_labels))\n",
        "train_actual_pred_df = pd.DataFrame(data=train_actual_pred, columns=['Train_actual_label', 'Train_pred_label'])\n",
        "train_actual_pred_df\n",
        "#train_actual_pred_df.to_csv('ML_results/BNN_Updated_trainData_Perform.csv', index=False)"
      ],
      "metadata": {
        "id": "fQmImkc1BqOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----- Save Different Performance Matrices - On Train DataSet\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, precision_score, accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
        "\n",
        "# Compute precision, accuracy, F1-score, and MCC for the predicted and true labels\n",
        "precision = precision_score(Y_train, train_majority_labels, average='macro')\n",
        "accuracy = accuracy_score(Y_train, train_majority_labels)\n",
        "f1_score = f1_score(Y_train, train_majority_labels, average='macro')\n",
        "mcc = matthews_corrcoef(Y_train, train_majority_labels)\n",
        "\n",
        "# Calculate sensitivity and specificity using confusion matrix\n",
        "cm = confusion_matrix(Y_train, train_majority_labels)\n",
        "\n",
        "# Calculate sensitivity and specificity for each class\n",
        "sensitivity = {}\n",
        "specificity = {}\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    tp = cm[i, i]\n",
        "    fn = sum(cm[i, :]) - tp\n",
        "    fp = sum(cm[:, i]) - tp\n",
        "    tn = cm.sum() - (tp + fn + fp)\n",
        "\n",
        "    sensitivity[i] = tp / (tp + fn)\n",
        "    specificity[i] = tn / (tn + fp)\n",
        "\n",
        "# Calculate overall sensitivity and specificity\n",
        "overall_sensitivity = sum(sensitivity.values()) / len(sensitivity)\n",
        "overall_specificity = sum(specificity.values()) / len(specificity)\n",
        "\n",
        "# Print the performance metrics to the console\n",
        "print(\"**************** MODEL PERFORMANCE: Actual BNN Train Dataset ****************\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Sensitivity:\")\n",
        "for key, value in sensitivity.items():\n",
        "    print(\"Class {}: {:.2f}\".format(key, value))\n",
        "print(\"Specificity:\")\n",
        "for key, value in specificity.items():\n",
        "    print(\"Class {}: {:.2f}\".format(key, value))\n",
        "print(\"Overall Sensitivity: {:.2f}\".format(overall_sensitivity))\n",
        "print(\"Overall Specificity: {:.2f}\".format(overall_specificity))\n",
        "\n",
        "print(\"Precision: {:.2f}\".format(precision))\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
        "print(\"F1-score: {:.2f}\".format(f1_score))\n",
        "print(\"MCC: {:.2f}\".format(mcc))"
      ],
      "metadata": {
        "id": "GuBf36aEBtlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Probabilites Data Frame\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "train_probabilities_df = pd.DataFrame()\n",
        "# Iterate over the instances and extract probabilities\n",
        "class_index = 0  # Index of the desired class\n",
        "for i in range(147):\n",
        "    instance_probs = probs(train_predictions_array, i) # This pobs functions was called while summarizing the predictions on the Test Dataset\n",
        "    train_probabilities_df = train_probabilities_df.append(pd.Series(instance_probs), ignore_index=True)\n",
        "\n",
        "# Set column names for the probabilities DataFrame\n",
        "train_probabilities_df.columns = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# Print the probabilities DataFrame\n",
        "print(train_probabilities_df)"
      ],
      "metadata": {
        "id": "INHMaA-DBzjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Prediction Results of BNN model on Train Dataset\n",
        "# train_actual_pred_df - has actual and predicted lables\n",
        "# print (train_actual_pred_df)\n",
        "\n",
        "# train_actual_pred_df - has all three probabilities\n",
        "# print(train_probabilities_df)\n",
        "\n",
        "#- Get the Drug names from Test Dataset and Scaler values of All variables\n",
        "train_data_drug_names = train_data[['Drug']]\n",
        "\n",
        "#--- Merge Y_Test_Labels, Y_preds, Y_preds_Proabilites and test Dataset information also\n",
        "BNN_trainData_concatenated_df = pd.concat([train_actual_pred_df, train_probabilities_df, train_data_drug_names, X_train], axis=1)\n",
        "print (BNN_trainData_concatenated_df)\n",
        "\n",
        "BNN_trainData_concatenated_df.to_csv('ML_results/BNN_trainData_Predictions.csv', index=False)  # Specify the desired file name and path"
      ],
      "metadata": {
        "id": "CXVPWlGEB3MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction probabilites figures on the Train DataSet for each Molecule"
      ],
      "metadata": {
        "id": "i9l-o78ACCX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### -- Prediction probabilites figures on the Test DataSet for each Molecule\n",
        "\n",
        "#-- This Code is to Bar Plot the Prediction probabilites for Each Compound Along with the Dot plot all Assay Parameters Associated\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example dataframe\n",
        "#df = pd.DataFrame({\n",
        "#    'Compound': ['A', 'B', 'C'],\n",
        "#    'Class 1': [0.6, 0.2, 0.4],\n",
        "#    'Class 2': [0.3, 0.1, 0.5],\n",
        "#    'Class 3': [0.1, 0.7, 0.1],\n",
        "#    'Y_test': ['Class 1', 'Class 3', 'Class 1'],\n",
        "#    'bsep': [100, 300, 400],\n",
        "#    'lop': [3, -4, 1],\n",
        "#    'cmax': [150, 20, 100],\n",
        "#    'glu': [1, 0.1, 0.5],\n",
        "#    'glu-gal': [20, 70, 50],\n",
        "#    'fsp3': [1000, 150, 800]\n",
        "#})\n",
        "\n",
        "#BNN_trainData_concatenated_df <- pd.read_csv(\"ML_results/BNN_trainData_Predictions.csv\")\n",
        "# Reoder the Columns in the concatenated_poly_df file\n",
        "df = BNN_trainData_concatenated_df.loc[:, ['Drug','Class 0','Class 1','Class 2','Train_actual_label','ClogP','BSEP','Glu','Glu_Gal','THLE','HepG2','Fsp3','log10cmax']]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.2\n",
        "\n",
        "# Set the positions of the bars on the x-axis\n",
        "bar_positions = np.arange(len(df.columns[1:4]))\n",
        "\n",
        "# Create a color map for the prediction probabilities\n",
        "color_map = ['#008b00', '#b8860b', '#b22222']\n",
        "\n",
        "#--- Get the Value Ranges to Add on the Dot Plot\n",
        "range_cols = df.columns[5:]\n",
        "# Set the range for the dot plot\n",
        "x_range = [df[range_cols].min().min(), df[range_cols].max().max()]\n",
        "x_min = df[range_cols].values.min()\n",
        "x_max = df[range_cols].values.max()\n",
        "\n",
        "# Loop over each compound and save the plot as an image\n",
        "for i, compound in enumerate(df['Drug']):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Bar Plot\n",
        "    prediction_probs = df.iloc[i, 1:4]\n",
        "    ax1.bar(bar_positions, prediction_probs, width=bar_width, color=color_map)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax1.set_xticks(bar_positions)\n",
        "    ax1.set_xticklabels(df.columns[1:4])\n",
        "    ax1.set_xlabel('')\n",
        "    ax1.set_ylabel('Prediction Probability')\n",
        "    ax1.set_title(f'Compound name- {compound} - (True Label: {df[\"Train_actual_label\"].iloc[i]})')\n",
        "\n",
        "    # Dot Plot\n",
        "    for j, col in enumerate(df.columns[5:]):\n",
        "        if col in range_cols:\n",
        "            ax2.plot([df[col].iloc[i]], [j], 'o', color='blue')  # Dot plot\n",
        "\n",
        "    for j, col in enumerate(range_cols):\n",
        "        ax2.hlines(y=j, xmin=df[col].min(), xmax=df[col].max(), colors='gray', linestyles='dashed', linewidth=1)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax2.set_yticks(range(len(range_cols)))\n",
        "    ax2.set_yticklabels(range_cols)\n",
        "    ax2.set_xlabel('Values')\n",
        "    ax2.set_ylabel('Variables')\n",
        "    ax2.set_title(f'Compound name {compound}')\n",
        "    ax2.set_xlim(x_min, x_max)\n",
        "\n",
        "    # Save the plot as an image\n",
        "    plt.subplots_adjust(wspace=0.5)\n",
        "    plt.savefig(f'ML_results/TrainData_Predictions_BNN/{compound}_plot.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "WkfFNJAlB8DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---- Export the Animation Out of the Images\n",
        "import os\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Directory containing the images\n",
        "image_dir = '~/ML_results/Predictions_BNN/train_data/'\n",
        "\n",
        "# Get a list of image file names in the directory\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "\n",
        "# Sort the image files by name (if necessary)\n",
        "image_files.sort()\n",
        "\n",
        "# Create a list to store the frames of the animation\n",
        "frames = []\n",
        "\n",
        "# Load each image and append it to the frames list\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    image = Image.open(image_path)\n",
        "    frames.append(image)\n",
        "\n",
        "# Create the animation (Play with the Duration to speed up or Slow the animation Speed)\n",
        "animation = Image.new('RGB', frames[0].size)\n",
        "animation.save('ML_results/BNN_train_Preds_Animation.gif', format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)"
      ],
      "metadata": {
        "id": "WTBJTc1DCe22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize Predictions on the Test DataSet"
      ],
      "metadata": {
        "id": "UagplkpjCxNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create an empty list to store the class label arrays\n",
        "class_labels = []\n",
        "\n",
        "# Iterate over the class labels\n",
        "for i in range(37):\n",
        "    class_label = posterior_pred[f'Y_test_{i}']\n",
        "    class_labels.append(class_label)\n",
        "\n",
        "# Stack the class label arrays horizontally\n",
        "test_predictions_array = np.hstack(class_labels)\n",
        "\n",
        "# Print the shape of the predictions array on Test Dataset\n",
        "print(test_predictions_array.shape)\n",
        "\n",
        "test_predictions_array"
      ],
      "metadata": {
        "id": "-k4bgAPqCz3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# posterior prediction for each category\n",
        "def probs(y_pred_df, ind):\n",
        "    y_pred_ind = y_pred_df[:,ind]\n",
        "    p = [sum(y_pred_ind == 0)/len(y_pred_ind), sum(y_pred_ind == 1)/len(y_pred_ind), sum(y_pred_ind == 2)/len(y_pred_ind)]\n",
        "    return p\n",
        "\n",
        "# compute posterior probabilities for a selected drug\n",
        "print(probs(test_predictions_array, 0))\n",
        "print(probs(test_predictions_array, 1))\n",
        "print(probs(test_predictions_array, 2))"
      ],
      "metadata": {
        "id": "GSVIQR4NC3IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize predictions On Test Data Set\n",
        "test_predictions_array\n",
        "test_predictions_array.shape # 1000 times 147 Columns (Data) - For Each compound get which class got majority votes\n",
        "test_predictions_array_df = pd.DataFrame(test_predictions_array)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Assuming you have a NumPy array called 'y_pred_train_df' with shape (5000, 147)\n",
        "# Each column contains three predicted labels\n",
        "\n",
        "# Convert the NumPy array to a DataFrame\n",
        "test_pred_df = pd.DataFrame(test_predictions_array_df)\n",
        "\n",
        "# Get the label with the majority of votes for each column\n",
        "test_majority_labels = np.asarray(mode(test_pred_df, axis=0)[0])[0]\n",
        "\n",
        "# Print the majority labels\n",
        "print(test_majority_labels)\n",
        "\n",
        "# Just check\n",
        "test_predictions_array_df[0].value_counts()"
      ],
      "metadata": {
        "id": "PBLUBlc_C8oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_actual_pred = np.column_stack((Y_test, test_majority_labels))\n",
        "test_actual_pred_df = pd.DataFrame(data=test_actual_pred, columns=['Y_test', 'Y_pred'])\n",
        "test_actual_pred_df\n",
        "#test_actual_pred_df.to_csv('ML_results/BNN_Updated_testData_Perform.csv', index=False)"
      ],
      "metadata": {
        "id": "jtgY2uwgDCd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----- Save Different Performance Matrices - On Test DataSet\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, precision_score, accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
        "\n",
        "# Compute precision, accuracy, F1-score, and MCC for the predicted and true labels\n",
        "precision = precision_score(Y_test, test_majority_labels, average='macro')\n",
        "accuracy = accuracy_score(Y_test, test_majority_labels)\n",
        "f1_score = f1_score(Y_test, test_majority_labels, average='macro')\n",
        "mcc = matthews_corrcoef(Y_test, test_majority_labels)\n",
        "\n",
        "# Calculate sensitivity and specificity using confusion matrix\n",
        "cm = confusion_matrix(Y_test, test_majority_labels)\n",
        "\n",
        "# Calculate sensitivity and specificity for each class\n",
        "sensitivity = {}\n",
        "specificity = {}\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    tp = cm[i, i]\n",
        "    fn = sum(cm[i, :]) - tp\n",
        "    fp = sum(cm[:, i]) - tp\n",
        "    tn = cm.sum() - (tp + fn + fp)\n",
        "\n",
        "    sensitivity[i] = tp / (tp + fn)\n",
        "    specificity[i] = tn / (tn + fp)\n",
        "\n",
        "# Calculate overall sensitivity and specificity\n",
        "overall_sensitivity = sum(sensitivity.values()) / len(sensitivity)\n",
        "overall_specificity = sum(specificity.values()) / len(specificity)\n",
        "\n",
        "# Print the performance metrics to the console\n",
        "print(\"**************** MODEL PERFORMANCE: Actual BNN Train Dataset ****************\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Sensitivity:\")\n",
        "for key, value in sensitivity.items():\n",
        "    print(\"Class {}: {:.2f}\".format(key, value))\n",
        "print(\"Specificity:\")\n",
        "for key, value in specificity.items():\n",
        "    print(\"Class {}: {:.2f}\".format(key, value))\n",
        "print(\"Overall Sensitivity: {:.2f}\".format(overall_sensitivity))\n",
        "print(\"Overall Specificity: {:.2f}\".format(overall_specificity))\n",
        "\n",
        "print(\"Precision: {:.2f}\".format(precision))\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
        "print(\"F1-score: {:.2f}\".format(f1_score))\n",
        "print(\"MCC: {:.2f}\".format(mcc))"
      ],
      "metadata": {
        "id": "10GysM1xDFo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Probabilites Data Frame\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "test_probabilities_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over the instances and extract probabilities\n",
        "class_index = 0  # Index of the desired class\n",
        "for i in range(37):\n",
        "    instance_probs = probs(test_predictions_array, i) # This pobs functions was called while summarizing the predictions on the Test Dataset\n",
        "    test_probabilities_df = test_probabilities_df.append(pd.Series(instance_probs), ignore_index=True)\n",
        "\n",
        "# Set column names for the probabilities DataFrame\n",
        "test_probabilities_df.columns = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# Print the probabilities DataFrame\n",
        "print(test_probabilities_df)"
      ],
      "metadata": {
        "id": "tVnD4A6qDQh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Prediction Results of BNN model on Test Dataset\n",
        "# test_actual_pred_df - has actual and predicted lables\n",
        "# print (test_actual_pred_df)\n",
        "# test_probabilities_df - has all three probabilities\n",
        "# print(test_probabilities_df)\n",
        "#-- Get the Drug names from Test Dataset and Scaler values of All variables\n",
        "test_data_drug_names = test_data[['Drug']]\n",
        "\n",
        "#--- Merge Y_Test_Labels, Y_preds, Y_preds_Proabilites and test Dataset information also\n",
        "BNN_testData_concatenated_df = pd.concat([test_actual_pred_df, test_probabilities_df, test_data_drug_names, X_test], axis=1)\n",
        "print (BNN_testData_concatenated_df)\n",
        "\n",
        "BNN_testData_concatenated_df.to_csv('ML_results/BNN_testData_Predictions.csv', index=False)  # Specify the desired file name and path"
      ],
      "metadata": {
        "id": "UCY4h_ovDTti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction probabilites figures on the Test DataSet for each Molecule"
      ],
      "metadata": {
        "id": "9EMmymtXDeX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### -- Prediction probabilites figures on the Test DataSet for each Molecule\n",
        "\n",
        "#-- This Code is to Bar Plot the Prediction probabilites for Each Compound Along with the Dot plot all Assay Parameters Associated\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example dataframe\n",
        "#df = pd.DataFrame({\n",
        "#    'Compound': ['A', 'B', 'C'],\n",
        "#    'Class 1': [0.6, 0.2, 0.4],\n",
        "#    'Class 2': [0.3, 0.1, 0.5],\n",
        "#    'Class 3': [0.1, 0.7, 0.1],\n",
        "#    'Y_test': ['Class 1', 'Class 3', 'Class 1'],\n",
        "#    'bsep': [100, 300, 400],\n",
        "#    'lop': [3, -4, 1],\n",
        "#    'cmax': [150, 20, 100],\n",
        "#    'glu': [1, 0.1, 0.5],\n",
        "#    'glu-gal': [20, 70, 50],\n",
        "#    'fsp3': [1000, 150, 800]\n",
        "#})\n",
        "\n",
        "#BNN_concatenated_df <- pd.read_csv(\"ML_results/BNN_testData_Predictions.csv\")\n",
        "# Reoder the Columns in the concatenated_poly_df file\n",
        "df = BNN_concatenated_df.loc[:, ['Drug','Class 0','Class 1','Class 2','Y_test','ClogP','BSEP','Glu','Glu_Gal','THLE','HepG2','Fsp3','log10cmax']]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.2\n",
        "\n",
        "# Set the positions of the bars on the x-axis\n",
        "bar_positions = np.arange(len(df.columns[1:4]))\n",
        "\n",
        "# Create a color map for the prediction probabilities\n",
        "color_map = ['#008b00', '#b8860b', '#b22222']\n",
        "\n",
        "#--- Get the Value Ranges to Add on the Dot Plot\n",
        "range_cols = df.columns[5:]\n",
        "# Set the range for the dot plot\n",
        "x_range = [df[range_cols].min().min(), df[range_cols].max().max()]\n",
        "x_min = df[range_cols].values.min()\n",
        "x_max = df[range_cols].values.max()\n",
        "\n",
        "# Loop over each compound and save the plot as an image\n",
        "for i, compound in enumerate(df['Drug']):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Bar Plot\n",
        "    prediction_probs = df.iloc[i, 1:4]\n",
        "    ax1.bar(bar_positions, prediction_probs, width=bar_width, color=color_map)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax1.set_xticks(bar_positions)\n",
        "    ax1.set_xticklabels(df.columns[1:4])\n",
        "    ax1.set_xlabel('')\n",
        "    ax1.set_ylabel('Prediction Probability')\n",
        "    ax1.set_title(f'Compound name- {compound} - (True Label: {df[\"Y_test\"].iloc[i]})')\n",
        "\n",
        "    # Dot Plot\n",
        "    for j, col in enumerate(df.columns[5:]):\n",
        "        if col in range_cols:\n",
        "            ax2.plot([df[col].iloc[i]], [j], 'o', color='blue')  # Dot plot\n",
        "\n",
        "    for j, col in enumerate(range_cols):\n",
        "        ax2.hlines(y=j, xmin=df[col].min(), xmax=df[col].max(), colors='gray', linestyles='dashed', linewidth=1)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax2.set_yticks(range(len(range_cols)))\n",
        "    ax2.set_yticklabels(range_cols)\n",
        "    ax2.set_xlabel('Values')\n",
        "    ax2.set_ylabel('Variables')\n",
        "    ax2.set_title(f'Compound name {compound}')\n",
        "    ax2.set_xlim(x_min, x_max)\n",
        "\n",
        "    # Save the plot as an image\n",
        "    plt.subplots_adjust(wspace=0.5)\n",
        "    plt.savefig(f'ML_results/Predictions_BNN/test_data/{compound}_plot.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "tn_MWiYeDXAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---- Export the Animation Out of the Images\n",
        "import os\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Directory containing the images\n",
        "image_dir = '~/ML_results/Predictions_BNN/test_data/'\n",
        "\n",
        "# Get a list of image file names in the directory\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "\n",
        "# Sort the image files by name (if necessary)\n",
        "image_files.sort()\n",
        "\n",
        "# Create a list to store the frames of the animation\n",
        "frames = []\n",
        "\n",
        "# Load each image and append it to the frames list\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    image = Image.open(image_path)\n",
        "    frames.append(image)\n",
        "\n",
        "# Create the animation (Play with the Duration to speed up or Slow the animation Speed)\n",
        "animation = Image.new('RGB', frames[0].size)\n",
        "animation.save('ML_results/BNN_test_Preds_Animation.gif', format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)"
      ],
      "metadata": {
        "id": "1aCfaRsiDpTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "\n",
        "# Create a dictionary of variables you want to save\n",
        "workspace = {\n",
        "    #'posterior_pred': posterior_pred,\n",
        "   # 'trace': trace,\n",
        "    #'model': model,\n",
        "    'train_predictions_array': train_predictions_array,\n",
        "    'test_predictions_array': test_predictions_array,\n",
        "    'train_actual_pred_df':train_actual_pred_df,\n",
        "    'test_actual_pred_df':test_actual_pred_df,\n",
        "    'train_majority_labels': train_majority_labels,\n",
        "    'test_majority_labels': test_majority_labels,\n",
        "    'train_probabilities_df': train_probabilities_df,\n",
        "    'test_probabilities_df': test_probabilities_df,\n",
        "    'BNN_trainData_concatenated_df': BNN_trainData_concatenated_df,\n",
        "    'BNN_testData_concatenated_df': BNN_testData_concatenated_df\n",
        "    # Add other variables here\n",
        "}\n",
        "\n",
        "# Save the workspace\n",
        "with open('Final_models/BNN_updated_Workspace.pkl', 'wb') as file:\n",
        "    dill.dump(workspace, file)"
      ],
      "metadata": {
        "id": "rnjf5Q_uDqPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}